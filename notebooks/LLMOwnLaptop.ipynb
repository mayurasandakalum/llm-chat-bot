{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3be2c3d7-e637-4ca0-b823-1442ce1ef014",
      "metadata": {
        "id": "3be2c3d7-e637-4ca0-b823-1442ce1ef014"
      },
      "source": [
        "# Running an LLM on your own laptop\n",
        "In this notebook, we're going to learn how to run a Hugging Face LLM on our own machine."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c88379d-de11-4f6b-b3b2-dd1c86ec1f2d",
      "metadata": {
        "id": "0c88379d-de11-4f6b-b3b2-dd1c86ec1f2d"
      },
      "source": [
        "## Download the LLM\n",
        "We're going to write some code to manually download the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eba955c-10f9-435a-8f5c-a4f3148214f6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-31T21:47:04.712381Z",
          "iopub.status.busy": "2023-07-31T21:47:04.711940Z",
          "iopub.status.idle": "2023-07-31T21:47:04.716609Z",
          "shell.execute_reply": "2023-07-31T21:47:04.714945Z",
          "shell.execute_reply.started": "2023-07-31T21:47:04.712355Z"
        },
        "id": "5eba955c-10f9-435a-8f5c-a4f3148214f6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1769883-5d03-45a6-89a4-53d97a2b31cb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-31T21:47:11.801528Z",
          "iopub.status.busy": "2023-07-31T21:47:11.801087Z",
          "iopub.status.idle": "2023-07-31T21:47:11.806484Z",
          "shell.execute_reply": "2023-07-31T21:47:11.805533Z",
          "shell.execute_reply.started": "2023-07-31T21:47:11.801503Z"
        },
        "id": "e1769883-5d03-45a6-89a4-53d97a2b31cb"
      },
      "outputs": [],
      "source": [
        "HUGGING_FACE_API_KEY = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d4a3cf-3aef-48a2-9d9c-c0e6f47b8ae9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-31T21:47:35.032925Z",
          "iopub.status.busy": "2023-07-31T21:47:35.032483Z",
          "iopub.status.idle": "2023-07-31T21:47:35.036911Z",
          "shell.execute_reply": "2023-07-31T21:47:35.035883Z",
          "shell.execute_reply.started": "2023-07-31T21:47:35.032900Z"
        },
        "id": "f5d4a3cf-3aef-48a2-9d9c-c0e6f47b8ae9"
      },
      "outputs": [],
      "source": [
        "model_id = \"Kaludi/Customer-Support-Assistant-V2\"\n",
        "filenames = [\n",
        "        \"config.json\", \"generation_config.json\", \"merges.txt\", \"special_tokens_map.json\",\n",
        "        \"tf_model.h5\", \"tokenizer.json\", \"tokenizer_config.json\", \"vocab.json\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de36f22-0e89-4add-9c2b-bbb34c4d9d26",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-31T21:47:59.770152Z",
          "iopub.status.busy": "2023-07-31T21:47:59.769784Z",
          "iopub.status.idle": "2023-07-31T21:48:00.700964Z",
          "shell.execute_reply": "2023-07-31T21:48:00.700440Z",
          "shell.execute_reply.started": "2023-07-31T21:47:59.770133Z"
        },
        "id": "8de36f22-0e89-4add-9c2b-bbb34c4d9d26"
      },
      "outputs": [],
      "source": [
        "for filename in filenames:\n",
        "        downloaded_model_path = hf_hub_download(\n",
        "                    repo_id=model_id,\n",
        "                    filename=filename,\n",
        "                    token=HUGGING_FACE_API_KEY\n",
        "        )\n",
        "        print(downloaded_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gjusu8097np3",
      "metadata": {
        "id": "gjusu8097np3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "pipeline = pipeline(\"text2text-generation\", model=model, device=-1, tokenizer=tokenizer, max_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "izZhUNvA75lW",
      "metadata": {
        "id": "izZhUNvA75lW"
      },
      "outputs": [],
      "source": [
        "pipeline(\"I want a help to fix my laptop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fAqWGXyFBRa6",
      "metadata": {
        "id": "fAqWGXyFBRa6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3jPcIAQo-z_f",
      "metadata": {
        "id": "3jPcIAQo-z_f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "HUGGING_FACE_API_KEY = \"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\"\n",
        "\n",
        "model_id = \"Kaludi/Customer-Support-Assistant-V2\"\n",
        "filenames = [\n",
        "        \"config.json\", \"generation_config.json\", \"merges.txt\", \"special_tokens_map.json\",\n",
        "        \"tf_model.h5\", \"tokenizer.json\", \"tokenizer_config.json\", \"vocab.json\"\n",
        "]\n",
        "\n",
        "for filename in filenames:\n",
        "        downloaded_model_path = hf_hub_download(\n",
        "                    repo_id=model_id,\n",
        "                    filename=filename,\n",
        "                    token=HUGGING_FACE_API_KEY\n",
        "        )\n",
        "        print(downloaded_model_path)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "pipeline = pipeline(\"text2text-generation\", model=model, device=-1, tokenizer=tokenizer, max_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3xuT8lpvBSX7",
      "metadata": {
        "id": "3xuT8lpvBSX7"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cYuDifs_Gro5",
      "metadata": {
        "id": "cYuDifs_Gro5"
      },
      "source": [
        "Kaludi/Customer-Support-Assistant-V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66a-B5gbAtVh",
      "metadata": {
        "id": "66a-B5gbAtVh"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from huggingface_hub import hf_hub_download\n",
        "# from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# HUGGING_FACE_API_KEY = \"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\"\n",
        "# model_id = \"Kaludi/Customer-Support-Assistant-V2\"\n",
        "# filenames = [\n",
        "#     \"config.json\", \"generation_config.json\", \"merges.txt\", \"special_tokens_map.json\",\n",
        "#     \"tf_model.h5\", \"tokenizer.json\", \"tokenizer_config.json\", \"vocab.json\"\n",
        "# ]\n",
        "\n",
        "# for filename in filenames:\n",
        "#     downloaded_model_path = hf_hub_download(\n",
        "#                 repo_id=model_id,\n",
        "#                 filename=filename,\n",
        "#                 token=HUGGING_FACE_API_KEY\n",
        "#     )\n",
        "#     print(downloaded_model_path)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "# model = TFAutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "# text_generator = pipeline(\"text2text-generation\", model=model, device=-1, tokenizer=tokenizer, max_length=1000)\n",
        "\n",
        "# def chat(user_input):\n",
        "#     # Generate a response to the user's input\n",
        "#     model_response = text_generator(user_input)[0]['generated_text']\n",
        "#     return model_response\n",
        "\n",
        "# # Example usage:\n",
        "# print(chat(\"Hello, how can I help you?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pbBicavRB-u_",
      "metadata": {
        "id": "pbBicavRB-u_"
      },
      "outputs": [],
      "source": [
        "# print(chat(\"Yes sure, this is my email, mayura@gmail.com\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mWCPfaNpFjlC",
      "metadata": {
        "id": "mWCPfaNpFjlC"
      },
      "outputs": [],
      "source": [
        "# print(chat(\"order number means?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ctDvHEfqG3Ey",
      "metadata": {
        "id": "ctDvHEfqG3Ey"
      },
      "source": [
        "hmzkhnswt/tinyllama_customerSupport_hmc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f11abec",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "HUGGING_FACE_API_KEY = \"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\"\n",
        "model_id = \"hmzkhnswt/tinyllama_customerSupport_hmc\"\n",
        "filenames = [\n",
        "    \"config.json\", \"generation_config.json\", \"model.safetensors\",\n",
        "    \"special_tokens_map.json\", \"tokenizer.json\", \"tokenizer.model\",\n",
        "    \"tokenizer_config.json\"\n",
        "]\n",
        "\n",
        "for filename in filenames:\n",
        "    downloaded_model_path = hf_hub_download(\n",
        "                repo_id=model_id,\n",
        "                filename=filename,\n",
        "                token=HUGGING_FACE_API_KEY\n",
        "    )\n",
        "    print(downloaded_model_path)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31e01c10",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(user_input):\n",
        "    # Generate a response to the user's input\n",
        "    model_response = text_generator(user_input)[0]['generated_text']\n",
        "    return model_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "549cbf02",
      "metadata": {},
      "outputs": [],
      "source": [
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    # If the user types 'quit', end the conversation\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Generate a response from the model\n",
        "    response = chat(user_input)\n",
        "\n",
        "    # Print the model's response\n",
        "    print(\"AI: \", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "visOlLC0OL_w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "visOlLC0OL_w",
        "outputId": "94834355-46da-4191-a132-1e812cdc24cd"
      },
      "outputs": [],
      "source": [
        "print(chat(\"I want to cancel my order\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8nE5OTamOmYY",
      "metadata": {
        "id": "8nE5OTamOmYY"
      },
      "outputs": [],
      "source": [
        "print(chat(\"Serial number: 557XtGs\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eCE5OSqIR6p4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCE5OSqIR6p4",
        "outputId": "ab3fd8e5-8cef-4a98-da97-79086956cc86"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    # If the user types 'quit', end the conversation\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Generate a response from the model\n",
        "    response = chat(user_input)\n",
        "\n",
        "    # Print the model's response\n",
        "    print(\"AI: \", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e27e4db",
      "metadata": {},
      "source": [
        "---------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d77f514",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "HUGGING_FACE_API_KEY = \"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\"\n",
        "model_id = \"matrrix/gpt2-customer-service\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db504123",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(user_input):\n",
        "    # Generate a response to the user's input\n",
        "    model_response = text_generator(user_input)[0]['generated_text']\n",
        "    return model_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7dd0965",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(chat(\"I want to cancel my order\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb2f686",
      "metadata": {},
      "outputs": [],
      "source": [
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    # If the user types 'quit', end the conversation\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Generate a response from the model\n",
        "    response = chat(user_input)\n",
        "\n",
        "    # Print the model's response\n",
        "    print(\"AI: \", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbef942a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "HUGGING_FACE_API_KEY = \"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\"\n",
        "model_id = \"ansilmbabl/ft-opt-125m-customer-chatbot-4-zephyr-tokenizer\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "020c2665",
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module transformers has no attribute TFBartForCausalLM",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKaludi/Customer-Support-Assistant-V2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, legacy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m text_generator \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    572\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\modeling_utils.py:3673\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3670\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3671\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_tf2_checkpoint_in_pytorch_model\n\u001b[1;32m-> 3673\u001b[0m     model, loading_info \u001b[38;5;241m=\u001b[39m \u001b[43mload_tf2_checkpoint_in_pytorch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m   3675\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3676\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   3677\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m   3678\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3679\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please see https://pytorch.org/ and https://www.tensorflow.org/install/ for installation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3680\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m instructions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3681\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:455\u001b[0m, in \u001b[0;36mload_tf2_checkpoint_in_pytorch_model\u001b[1;34m(pt_model, tf_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# Instantiate and load the associated TF 2.0 model\u001b[39;00m\n\u001b[0;32m    454\u001b[0m tf_model_class_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m pt_model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m  \u001b[38;5;66;03m# Add \"TF\" at the beginning\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m tf_model_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_model_class_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m tf_model \u001b[38;5;241m=\u001b[39m tf_model_class(pt_model\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1375\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1373\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
            "\u001b[1;31mAttributeError\u001b[0m: module transformers has no attribute TFBartForCausalLM"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "HUGGING_FACE_API_KEY = \"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\"\n",
        "model_id = \"Kaludi/Customer-Support-Assistant-V2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, from_tf=True)\n",
        "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "71354fe5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(user_input):\n",
        "    # Generate a response to the user's input\n",
        "    model_response = text_generator(user_input)[0]['generated_text']\n",
        "    return model_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f4423ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(chat(\"I want to cancel my order\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cf6be25e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "HUGGING_FACE_API_KEY = \"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\"\n",
        "model_id = \"ansilmbabl/ft-opt-125m-customer-chatbot-4-zephyr-tokenizer\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e996436f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(user_input):\n",
        "    model_response = text_generator(user_input, max_length=300, temperature=0.7)[0]['generated_text']\n",
        "    return model_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "357bfda3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\generation\\utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "c:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI want to cancel my order\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
            "Cell \u001b[1;32mIn[2], line 2\u001b[0m, in \u001b[0;36mchat\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(user_input):\n\u001b[1;32m----> 2\u001b[0m     model_response \u001b[38;5;241m=\u001b[39m \u001b[43mtext_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_response\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\pipelines\\base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         )\n\u001b[0;32m   1138\u001b[0m     )\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\pipelines\\base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\pipelines\\base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\generation\\utils.py:1718\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[0;32m   1702\u001b[0m         input_ids,\n\u001b[0;32m   1703\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1714\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1715\u001b[0m     )\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1719\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[0;32m   1732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\generation\\utils.py:2579\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2576\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2578\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2579\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2580\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2582\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2583\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2584\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2587\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:1155\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[0;32m   1144\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1145\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1153\u001b[0m )\n\u001b[1;32m-> 1155\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m   1157\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(chat(\"I want to cancel my order\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
