{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3be2c3d7-e637-4ca0-b823-1442ce1ef014",
      "metadata": {
        "id": "3be2c3d7-e637-4ca0-b823-1442ce1ef014"
      },
      "source": [
        "# Running an LLM on your own laptop\n",
        "In this notebook, we're going to learn how to run a Hugging Face LLM on our own machine."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c88379d-de11-4f6b-b3b2-dd1c86ec1f2d",
      "metadata": {
        "id": "0c88379d-de11-4f6b-b3b2-dd1c86ec1f2d"
      },
      "source": [
        "## Download the LLM\n",
        "We're going to write some code to manually download the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eba955c-10f9-435a-8f5c-a4f3148214f6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-31T21:47:04.712381Z",
          "iopub.status.busy": "2023-07-31T21:47:04.711940Z",
          "iopub.status.idle": "2023-07-31T21:47:04.716609Z",
          "shell.execute_reply": "2023-07-31T21:47:04.714945Z",
          "shell.execute_reply.started": "2023-07-31T21:47:04.712355Z"
        },
        "id": "5eba955c-10f9-435a-8f5c-a4f3148214f6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1769883-5d03-45a6-89a4-53d97a2b31cb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-31T21:47:11.801528Z",
          "iopub.status.busy": "2023-07-31T21:47:11.801087Z",
          "iopub.status.idle": "2023-07-31T21:47:11.806484Z",
          "shell.execute_reply": "2023-07-31T21:47:11.805533Z",
          "shell.execute_reply.started": "2023-07-31T21:47:11.801503Z"
        },
        "id": "e1769883-5d03-45a6-89a4-53d97a2b31cb"
      },
      "outputs": [],
      "source": [
        "HUGGING_FACE_API_KEY = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d4a3cf-3aef-48a2-9d9c-c0e6f47b8ae9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-31T21:47:35.032925Z",
          "iopub.status.busy": "2023-07-31T21:47:35.032483Z",
          "iopub.status.idle": "2023-07-31T21:47:35.036911Z",
          "shell.execute_reply": "2023-07-31T21:47:35.035883Z",
          "shell.execute_reply.started": "2023-07-31T21:47:35.032900Z"
        },
        "id": "f5d4a3cf-3aef-48a2-9d9c-c0e6f47b8ae9"
      },
      "outputs": [],
      "source": [
        "model_id = \"Kaludi/Customer-Support-Assistant-V2\"\n",
        "filenames = [\n",
        "        \"config.json\", \"generation_config.json\", \"merges.txt\", \"special_tokens_map.json\",\n",
        "        \"tf_model.h5\", \"tokenizer.json\", \"tokenizer_config.json\", \"vocab.json\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de36f22-0e89-4add-9c2b-bbb34c4d9d26",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-31T21:47:59.770152Z",
          "iopub.status.busy": "2023-07-31T21:47:59.769784Z",
          "iopub.status.idle": "2023-07-31T21:48:00.700964Z",
          "shell.execute_reply": "2023-07-31T21:48:00.700440Z",
          "shell.execute_reply.started": "2023-07-31T21:47:59.770133Z"
        },
        "id": "8de36f22-0e89-4add-9c2b-bbb34c4d9d26"
      },
      "outputs": [],
      "source": [
        "for filename in filenames:\n",
        "        downloaded_model_path = hf_hub_download(\n",
        "                    repo_id=model_id,\n",
        "                    filename=filename,\n",
        "                    token=HUGGING_FACE_API_KEY\n",
        "        )\n",
        "        print(downloaded_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gjusu8097np3",
      "metadata": {
        "id": "gjusu8097np3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "pipeline = pipeline(\"text2text-generation\", model=model, device=-1, tokenizer=tokenizer, max_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "izZhUNvA75lW",
      "metadata": {
        "id": "izZhUNvA75lW"
      },
      "outputs": [],
      "source": [
        "pipeline(\"I want a help to fix my laptop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fAqWGXyFBRa6",
      "metadata": {
        "id": "fAqWGXyFBRa6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3jPcIAQo-z_f",
      "metadata": {
        "id": "3jPcIAQo-z_f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "HUGGING_FACE_API_KEY = \"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\"\n",
        "\n",
        "model_id = \"Kaludi/Customer-Support-Assistant-V2\"\n",
        "filenames = [\n",
        "        \"config.json\", \"generation_config.json\", \"merges.txt\", \"special_tokens_map.json\",\n",
        "        \"tf_model.h5\", \"tokenizer.json\", \"tokenizer_config.json\", \"vocab.json\"\n",
        "]\n",
        "\n",
        "for filename in filenames:\n",
        "        downloaded_model_path = hf_hub_download(\n",
        "                    repo_id=model_id,\n",
        "                    filename=filename,\n",
        "                    token=HUGGING_FACE_API_KEY\n",
        "        )\n",
        "        print(downloaded_model_path)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "pipeline = pipeline(\"text2text-generation\", model=model, device=-1, tokenizer=tokenizer, max_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3xuT8lpvBSX7",
      "metadata": {
        "id": "3xuT8lpvBSX7"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cYuDifs_Gro5",
      "metadata": {
        "id": "cYuDifs_Gro5"
      },
      "source": [
        "Kaludi/Customer-Support-Assistant-V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66a-B5gbAtVh",
      "metadata": {
        "id": "66a-B5gbAtVh"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from huggingface_hub import hf_hub_download\n",
        "# from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# HUGGING_FACE_API_KEY = \"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\"\n",
        "# model_id = \"Kaludi/Customer-Support-Assistant-V2\"\n",
        "# filenames = [\n",
        "#     \"config.json\", \"generation_config.json\", \"merges.txt\", \"special_tokens_map.json\",\n",
        "#     \"tf_model.h5\", \"tokenizer.json\", \"tokenizer_config.json\", \"vocab.json\"\n",
        "# ]\n",
        "\n",
        "# for filename in filenames:\n",
        "#     downloaded_model_path = hf_hub_download(\n",
        "#                 repo_id=model_id,\n",
        "#                 filename=filename,\n",
        "#                 token=HUGGING_FACE_API_KEY\n",
        "#     )\n",
        "#     print(downloaded_model_path)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "# model = TFAutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "# text_generator = pipeline(\"text2text-generation\", model=model, device=-1, tokenizer=tokenizer, max_length=1000)\n",
        "\n",
        "# def chat(user_input):\n",
        "#     # Generate a response to the user's input\n",
        "#     model_response = text_generator(user_input)[0]['generated_text']\n",
        "#     return model_response\n",
        "\n",
        "# # Example usage:\n",
        "# print(chat(\"Hello, how can I help you?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pbBicavRB-u_",
      "metadata": {
        "id": "pbBicavRB-u_"
      },
      "outputs": [],
      "source": [
        "# print(chat(\"Yes sure, this is my email, mayura@gmail.com\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mWCPfaNpFjlC",
      "metadata": {
        "id": "mWCPfaNpFjlC"
      },
      "outputs": [],
      "source": [
        "# print(chat(\"order number means?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ctDvHEfqG3Ey",
      "metadata": {
        "id": "ctDvHEfqG3Ey"
      },
      "source": [
        "hmzkhnswt/tinyllama_customerSupport_hmc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f11abec",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "HUGGING_FACE_API_KEY = \"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\"\n",
        "model_id = \"hmzkhnswt/tinyllama_customerSupport_hmc\"\n",
        "filenames = [\n",
        "    \"config.json\", \"generation_config.json\", \"model.safetensors\",\n",
        "    \"special_tokens_map.json\", \"tokenizer.json\", \"tokenizer.model\",\n",
        "    \"tokenizer_config.json\"\n",
        "]\n",
        "\n",
        "for filename in filenames:\n",
        "    downloaded_model_path = hf_hub_download(\n",
        "                repo_id=model_id,\n",
        "                filename=filename,\n",
        "                token=HUGGING_FACE_API_KEY\n",
        "    )\n",
        "    print(downloaded_model_path)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31e01c10",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(user_input):\n",
        "    # Generate a response to the user's input\n",
        "    model_response = text_generator(user_input)[0]['generated_text']\n",
        "    return model_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "549cbf02",
      "metadata": {},
      "outputs": [],
      "source": [
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    # If the user types 'quit', end the conversation\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Generate a response from the model\n",
        "    response = chat(user_input)\n",
        "\n",
        "    # Print the model's response\n",
        "    print(\"AI: \", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "visOlLC0OL_w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "visOlLC0OL_w",
        "outputId": "94834355-46da-4191-a132-1e812cdc24cd"
      },
      "outputs": [],
      "source": [
        "print(chat(\"I want to cancel my order\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8nE5OTamOmYY",
      "metadata": {
        "id": "8nE5OTamOmYY"
      },
      "outputs": [],
      "source": [
        "print(chat(\"Serial number: 557XtGs\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eCE5OSqIR6p4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCE5OSqIR6p4",
        "outputId": "ab3fd8e5-8cef-4a98-da97-79086956cc86"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    # If the user types 'quit', end the conversation\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Generate a response from the model\n",
        "    response = chat(user_input)\n",
        "\n",
        "    # Print the model's response\n",
        "    print(\"AI: \", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e27e4db",
      "metadata": {},
      "source": [
        "---------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8d77f514",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "HUGGING_FACE_API_KEY = \"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\"\n",
        "model_id = \"matrrix/gpt2-customer-service\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "db504123",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(user_input):\n",
        "    # Generate a response to the user's input\n",
        "    model_response = text_generator(user_input)[0]['generated_text']\n",
        "    return model_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a7dd0965",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I want to cancel my order for my refunded order. Please contact my account and we will contact you to arrange a refund(s) before or after my account is cancelled. When I have already arranged my account for my refund(s), please contact me and try again for the next time. If it is still difficult to cancel my order, please contact me directly to set up account so i can contact my account in case my order is cancelled. Thanks.\n",
            "\n",
            "\n",
            "Please Note:\n",
            "\n",
            "When you cancel your order, you will receive your refund(s) and you can return it to us for a refund (see your return policy).\n",
            "\n",
            "\n",
            "When my order is shipped out, my billing address will not be updated. Thanks!!\n",
            "\n",
            "Please note in the return Policy my address is not available on my website\n",
            "\n",
            "\n",
            "What do i pay for the shipping and delivery?\n",
            "\n",
            "Shipping fees will be charged for the USA post office\n",
            "\n",
            "We will return your order for a refund on the first business day of the 30 day period, or within 3 Business Days before delivery date (unless for local issues):\n",
            "\n",
            "If you received the package in 2 business days or less:\n",
            "\n",
            "(if the package was last opened in 1 year) I will replace it with the refund(s)\n",
            "\n",
            "If it is not refund within 3 Business Days after the order is placed:\n",
            "\n",
            "I can no longer place an order with my regular address (please check) or in case of any problem, I will inform my customer of that (please contact your customer service service service)\n",
            "\n",
            "All other charges due(when applicable) will be deducted from my account & refund.\n",
            "\n",
            "Will I be charged on time?\n",
            "\n",
            "If a package is returned within 6 days and we receive that package from a different address or in a different carrier, then the package will be charged for the same time. We will refund the remaining postage(s) to you(if applicable) within 30 days by any method whatsoever without request of the customer, and i won't be charged again for that return postage!!\n",
            "\n",
            "\n",
            "My order shipped without my account?\n",
            "\n",
            "Because my order was shipped outside of USA I will receive a refund in the next 6 days.\n",
            "\n",
            "\n",
            "Return Policy (including return postage):\n",
            "\n",
            "We will pay the full order cost for your return delivery(s) to you (the invoice) for USA post Ai's or express shipping.\n",
            "\n",
            "If I forgot or failed to pay within 30 days after your order has been accepted via e-Check, in the return policy:\n",
            "\n",
            "Due to international customs, we will need to give you a 10-day tracking number and the order will cost you no more than 50-60 days.\n",
            "\n",
            "Returns Policy:\n",
            "\n",
            "After receiving a return you may return the package to you within 5 working days from receipt of the invoice.\n",
            "\n",
            "Please provide me with:\n",
            "\n",
            "Proof of delivery address\n",
            "\n",
            "Proof of my account (in case of domestic)\n",
            "\n",
            "Proof of payment method before returning the package\n",
            "\n",
            "Credit/Debit Card accepted.\n",
            "\n",
            "I do not have a credit or debit card account?\n",
            "\n",
            "All payments in bitcoin must be in bank or DHL credit or debit card approved account.\n",
            "\n",
            "How do i check my order status?\n",
            "\n",
            "We will check my order history before sending packages to you. After that, we will transfer/check to my account via credit or debit card.\n",
            "\n",
            "By purchasing an account in my PayPal Profile, I agree to the payment form required for PayPal Payment Service.\n",
            "\n",
            "Does my package arrived damaged?\n",
            "\n",
            "Our payment method will automatically take order to your credit/debit card.\n",
            "\n",
            "My order arrived damaged, i will have to send payment.\n",
            "\n",
            "What happens to the order?\n",
            "\n",
            "Our account will be temporarily suspended for the period of time required within which you receive your package.\n",
            "\n",
            "What can i do on PayPal?\n",
            "\n",
            "Due to postal service, your package will be delivered through an international postal service or a domestic carrier, i will not be able to receive package during the 30-day period.\n",
            "\n",
            "When to call or email?\n",
            "\n",
            "All messages sent by international carrier will be sent within 14 days from the purchase.\n",
            "\n",
            "Can my package be tracked/returned to me?\n",
            "\n",
            "Please be aware that return shipping cost is the same as USPS return shipping for all orders sold by USPS or APO/FPO/FPL.\n",
            "\n",
            "I order within 3 business days of receipt of order(s).\n",
            "\n",
            "I received an error message after 3 downloading steps(at time of order installation):\n",
            "\n",
            "please contact PayPal or APO/FPO/FPL with your payment problems.\n",
            "\n",
            "If the package was shipped late in the order process:please contact me directly.\n",
            "\n",
            "\n",
            "How do i cancel my order?\n",
            "\n",
            "After 2 business days of receiving receipt of package(s),\n"
          ]
        }
      ],
      "source": [
        "print(chat(\"I want to cancel my order\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb2f686",
      "metadata": {},
      "outputs": [],
      "source": [
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    # If the user types 'quit', end the conversation\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Generate a response from the model\n",
        "    response = chat(user_input)\n",
        "\n",
        "    # Print the model's response\n",
        "    print(\"AI: \", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bbef942a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "HUGGING_FACE_API_KEY = \"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\"\n",
        "model_id = \"ansilmbabl/ft-opt-125m-customer-chatbot-4-zephyr-tokenizer\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "71354fe5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(user_input):\n",
        "    # Generate a response to the user's input\n",
        "    model_response = text_generator(user_input)[0]['generated_text']\n",
        "    return model_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6f4423ae",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Makara PC\\.conda\\envs\\ml-practice\\Lib\\site-packages\\transformers\\generation\\utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "print(chat(\"I want to cancel my order\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
